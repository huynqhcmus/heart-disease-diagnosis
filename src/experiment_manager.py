#!/usr/bin/env python3
"""
Experiment Manager - Quáº£n lÃ½ thÃ­ nghiá»‡m ML
- Logging cáº¥u hÃ¬nh, tham sá»‘, káº¿t quáº£
- So sÃ¡nh káº¿t quáº£ giá»¯a cÃ¡c láº§n cháº¡y
- Reproducibility vá»›i seed cá»‘ Ä‘á»‹nh
"""

import os
import json
import pickle
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

class ExperimentManager:
    """Quáº£n lÃ½ thÃ­ nghiá»‡m ML vá»›i logging vÃ  reproducibility"""
    
    def __init__(self, experiment_name: str, base_dir: str = "experiments"):
        """
        Khá»Ÿi táº¡o Experiment Manager
        
        Args:
            experiment_name: TÃªn thÃ­ nghiá»‡m
            base_dir: ThÆ° má»¥c lÆ°u káº¿t quáº£
        """
        self.experiment_name = experiment_name
        self.base_dir = base_dir
        self.experiment_dir = os.path.join(base_dir, experiment_name)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Táº¡o thÆ° má»¥c thÃ­ nghiá»‡m
        os.makedirs(self.experiment_dir, exist_ok=True)
        os.makedirs(os.path.join(self.experiment_dir, "logs"), exist_ok=True)
        os.makedirs(os.path.join(self.experiment_dir, "models"), exist_ok=True)
        os.makedirs(os.path.join(self.experiment_dir, "results"), exist_ok=True)
        
        # Setup logging
        self._setup_logging()
        
        # LÆ°u metadata thÃ­ nghiá»‡m
        self.metadata = {
            "experiment_name": experiment_name,
            "timestamp": self.timestamp,
            "base_dir": base_dir
        }
        
    def _setup_logging(self):
        """Setup logging system"""
        log_file = os.path.join(self.experiment_dir, "logs", f"experiment_{self.timestamp}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ðŸš€ Báº¯t Ä‘áº§u thÃ­ nghiá»‡m: {self.experiment_name}")
        
    def set_seed(self, seed: int = 42):
        """Äáº·t seed cá»‘ Ä‘á»‹nh Ä‘á»ƒ Ä‘áº£m báº£o reproducibility"""
        np.random.seed(seed)
        os.environ["PYTHONHASHSEED"] = str(seed)
        
        self.metadata["seed"] = seed
        self.logger.info(f"ðŸŒ± Äáº·t seed: {seed}")
        
    def log_config(self, config: Dict[str, Any]):
        """Log cáº¥u hÃ¬nh thÃ­ nghiá»‡m"""
        self.metadata["config"] = config
        
        # LÆ°u config vÃ o file JSON
        config_file = os.path.join(self.experiment_dir, "config.json")
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"ðŸ“ ÄÃ£ lÆ°u cáº¥u hÃ¬nh: {config_file}")
        
    def log_parameters(self, model_name: str, params: Dict[str, Any]):
        """Log tham sá»‘ model"""
        if "model_parameters" not in self.metadata:
            self.metadata["model_parameters"] = {}
            
        self.metadata["model_parameters"][model_name] = params
        self.logger.info(f"âš™ï¸  Tham sá»‘ {model_name}: {params}")
        
    def log_results(self, model_name: str, results: Dict[str, float], 
                   train_time: float = None, test_predictions: np.ndarray = None):
        """Log káº¿t quáº£ thÃ­ nghiá»‡m"""
        if "results" not in self.metadata:
            self.metadata["results"] = {}
            
        result_entry = {
            "metrics": results,
            "train_time": train_time,
            "timestamp": datetime.now().isoformat()
        }
        
        self.metadata["results"][model_name] = result_entry
        
        # LÆ°u predictions náº¿u cÃ³
        if test_predictions is not None:
            pred_file = os.path.join(self.experiment_dir, "results", f"{model_name}_predictions.npy")
            np.save(pred_file, test_predictions)
            
        self.logger.info(f"ðŸ“Š Káº¿t quáº£ {model_name}: {results}")
        
    def save_model(self, model, model_name: str):
        """LÆ°u model Ä‘Ã£ train"""
        model_file = os.path.join(self.experiment_dir, "models", f"{model_name}.pkl")
        with open(model_file, 'wb') as f:
            pickle.dump(model, f)
            
        self.logger.info(f"ðŸ’¾ ÄÃ£ lÆ°u model: {model_file}")
        
    def compare_experiments(self, other_experiment_dir: str = None):
        """So sÃ¡nh vá»›i thÃ­ nghiá»‡m khÃ¡c"""
        if other_experiment_dir is None:
            # So sÃ¡nh vá»›i thÃ­ nghiá»‡m trÆ°á»›c Ä‘Ã³
            experiment_dirs = [d for d in os.listdir(self.base_dir) 
                             if os.path.isdir(os.path.join(self.base_dir, d))]
            if len(experiment_dirs) > 1:
                other_experiment_dir = os.path.join(self.base_dir, experiment_dirs[-2])
        
        if other_experiment_dir and os.path.exists(other_experiment_dir):
            # Load káº¿t quáº£ thÃ­ nghiá»‡m khÃ¡c
            other_config_file = os.path.join(other_experiment_dir, "config.json")
            if os.path.exists(other_config_file):
                with open(other_config_file, 'r') as f:
                    other_config = json.load(f)
                    
                self.logger.info(f"ðŸ”„ So sÃ¡nh vá»›i thÃ­ nghiá»‡m: {other_experiment_dir}")
                return self._create_comparison_report(other_config)
        
        return None
        
    def _create_comparison_report(self, other_config: Dict):
        """Táº¡o bÃ¡o cÃ¡o so sÃ¡nh"""
        comparison = {
            "current_experiment": self.metadata,
            "previous_experiment": other_config,
            "comparison_timestamp": datetime.now().isoformat()
        }
        
        comparison_file = os.path.join(self.experiment_dir, "comparison_report.json")
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump(comparison, f, indent=2, ensure_ascii=False)
            
        return comparison
        
    def finalize_experiment(self):
        """HoÃ n táº¥t thÃ­ nghiá»‡m vÃ  lÆ°u metadata"""
        self.metadata["status"] = "completed"
        self.metadata["end_time"] = datetime.now().isoformat()
        
        # LÆ°u metadata tá»•ng há»£p
        metadata_file = os.path.join(self.experiment_dir, "metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"âœ… HoÃ n táº¥t thÃ­ nghiá»‡m: {self.experiment_name}")
        self.logger.info(f"ðŸ“ Káº¿t quáº£ lÆ°u táº¡i: {self.experiment_dir}")
        
    def get_best_model(self) -> Optional[Dict]:
        """Láº¥y model cÃ³ káº¿t quáº£ tá»‘t nháº¥t"""
        if "results" not in self.metadata:
            return None
            
        best_model = None
        best_score = -1
        
        for model_name, result in self.metadata["results"].items():
            # Sá»­ dá»¥ng accuracy lÃ m metric chÃ­nh
            if "accuracy" in result["metrics"]:
                score = result["metrics"]["accuracy"]
                if score > best_score:
                    best_score = score
                    best_model = {
                        "name": model_name,
                        "score": score,
                        "metrics": result["metrics"]
                    }
                    
        return best_model

def create_enhanced_grid_search(estimator, param_grid: Dict, X, y, 
                              cv_splits: int = 5, scoring: str = 'accuracy',
                              experiment_manager: ExperimentManager = None):
    """GridSearchCV má»Ÿ rá»™ng vá»›i logging"""
    
    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)
    
    grid_search = GridSearchCV(
        estimator=estimator,
        param_grid=param_grid,
        cv=cv,
        scoring=scoring,
        n_jobs=-1,
        verbose=1
    )
    
    start_time = datetime.now()
    grid_search.fit(X, y)
    end_time = datetime.now()
    
    train_time = (end_time - start_time).total_seconds()
    
    if experiment_manager:
        experiment_manager.log_parameters("grid_search", {
            "param_grid": param_grid,
            "cv_splits": cv_splits,
            "scoring": scoring
        })
        
        experiment_manager.log_results("grid_search", {
            "best_score": grid_search.best_score_,
            "best_params": grid_search.best_params_,
            "cv_scores": grid_search.cv_results_['mean_test_score'].tolist()
        }, train_time)
        
        experiment_manager.save_model(grid_search.best_estimator_, "best_model")
    
    return grid_search, train_time
